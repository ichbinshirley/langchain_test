import os
import chromadb
from pathlib import Path
from llama_index import download_loader
from llama_index import VectorStoreIndex
from llama_index import ServiceContext
from llama_index.vector_stores import ChromaVectorStore
from llama_index import StorageContext
from llama_index.llms import OpenAI

# 设置openai api key
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

# 加载pdf文档
PDFReader = download_loader('PDFReader')
loader = PDFReader()
documents = loader.load_data(file=Path('your file path'))


# 文档分割，默认embedding model是openai，指定llm为gpt 3.5turbo
# service context 在llamaindex用于存放index和query的类
service_context = ServiceContext.from_defaults(chunk_size=500, llm=OpenAI())

# 向量存储，向量数据库为chroma
chroma_client = chromadb.PersistentClient()
chroma_collection = chroma_client.create_collection('llamaindex_test1')
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)   
storage_context = StorageContext.from_defaults(vector_store=vector_store) # storage context在llamaindex用于存放nodes类

# 创建索引
index = VectorStoreIndex.from_documents(documents,
                                        service_context=service_context,
                                        storage_context=storage_context)

# 指定响应模式，启用流式响应
query_engine = index.as_query_engine(response_mode='tree_summarize', streaming=True)
response = query_engine.query('your question')
response.print_response_stream()

